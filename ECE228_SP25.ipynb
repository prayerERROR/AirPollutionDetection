{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9904VYvPSh0F",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# install requirments\n",
    "# It is run on Colab T4 GPU\n",
    "!pip install timm\n",
    "!pip install kaggle\n",
    "!pip install datasets\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRMIgKAMSsKm"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import sys\n",
    "import timm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50, ResNet50_Weights, densenet121, DenseNet121_Weights\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHo5YYcfTPVW"
   },
   "outputs": [],
   "source": [
    "# set seed and device\n",
    "def set_seed(seed=228):\n",
    "  '''It is the final project of ECE228. So I choose 228 to be the seed.'''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtWE68LHTUgX"
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "# Kaggle key is needed\n",
    "def download_dataset():\n",
    "    if not os.path.exists(\"IMAGES\"):\n",
    "        print(\"Downloading dataset...\")\n",
    "        from google.colab import files\n",
    "\n",
    "        print(\"Please upload your kaggle.json file:\")\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "        os.system(\"cp kaggle.json ~/.kaggle/\")\n",
    "        os.system(\"chmod 600 ~/.kaggle/kaggle.json\")\n",
    "\n",
    "        os.system(\"kaggle datasets download -q adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal\")\n",
    "\n",
    "        # Extract dataset based on its dir tree\n",
    "        os.system('unzip -q air-pollution-image-dataset-from-india-and-nepal.zip \"Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_NEP/*\"')\n",
    "        os.system('mv \"Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_NEP\" .')\n",
    "        os.system('mv IND_and_NEP IMAGES')\n",
    "        os.system('rm -r \"Air Pollution Image Dataset/\" air-pollution-image-dataset-from-india-and-nepal.zip')\n",
    "\n",
    "        print(\"Dataset downloaded and organized!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists!\")\n",
    "\n",
    "\n",
    "download_dataset()\n",
    "\n",
    "# Check dataset structure\n",
    "classes = sorted(os.listdir(\"IMAGES\"))\n",
    "print(f\"Found classes: {classes}\")\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Show samples per class\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(\"IMAGES\", class_name)\n",
    "    num_samples = len(os.listdir(class_path))\n",
    "    print(f\"{class_name}: {num_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJwmXNmFTnGM"
   },
   "outputs": [],
   "source": [
    "# preprocess and load data\n",
    "\n",
    "def get_transforms(image_size=224):\n",
    "    \"\"\"0.5 is a not bad choice to normalize data when we do not know dataset well\"\"\"\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "def create_data_loaders(image_size=224, batch_size=32):\n",
    "    \"\"\"Create train and validation data loaders\"\"\"\n",
    "    train_transforms, val_transforms = get_transforms(image_size)\n",
    "\n",
    "    # Load dataset\n",
    "    full_dataset = ImageFolder(\"IMAGES\")\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    targets = [full_dataset.targets[i] for i in indices]\n",
    "\n",
    "    # Stratified split, still use 228 to be random seed\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, test_size=0.2, random_state=228, stratify=targets\n",
    "    )\n",
    "\n",
    "    train_dataset = ImageFolder(\"IMAGES\", transform=train_transforms)\n",
    "    val_dataset = ImageFolder(\"IMAGES\", transform=val_transforms)\n",
    "\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    # Note: you may need to modify parameter to run faster.\n",
    "    # Colab may have some issues when setting pin_memory True\n",
    "    train_loader = DataLoader(\n",
    "        train_subset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_subset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_subset)}\")\n",
    "    print(f\"Validation samples: {len(val_subset)}\")\n",
    "\n",
    "    return train_loader, val_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX8Xjr9uTsUj"
   },
   "outputs": [],
   "source": [
    "# define models: ResNet50, DenseNet121, EfficientNet-B0, Vision Transformer\n",
    "class AirPollutionResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "class AirPollutionDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.densenet = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        in_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)\n",
    "\n",
    "class AirPollutionEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        in_features = self.efficientnet.classifier.in_features\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "class AirPollutionViT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        in_features = self.vit.head.in_features\n",
    "        self.vit.head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLvWdB4OT3xH"
   },
   "outputs": [],
   "source": [
    "# functions used during training\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def train_model(model, train_loader, val_loader, model_name, num_epochs=15):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'best_{model_name.lower()}_model.pth')\n",
    "            print(f\"New best model saved! Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return model, history, best_val_acc\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_loader, model_name, classes):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    _, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Final Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=classes, digits=4))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXl15fcfIWsj"
   },
   "outputs": [],
   "source": [
    "# main pipeline\n",
    "def main():\n",
    "    print(\"Starting Air Pollution Detection Training Pipeline\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Load data\n",
    "    train_loader, val_loader, classes = create_data_loaders(image_size=224, batch_size=32)\n",
    "\n",
    "    # Define the models used\n",
    "    models_to_train = [\n",
    "        (\"ResNet50\", AirPollutionResNet),\n",
    "        (\"DenseNet121\", AirPollutionDenseNet),\n",
    "        (\"EfficientNet-B0\", AirPollutionEfficientNet),\n",
    "        (\"ViT\", AirPollutionViT)\n",
    "    ]\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model_class in models_to_train:\n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"TRAINING {model_name}\")\n",
    "            print(f\"{'='*80}\")\n",
    "\n",
    "            model = model_class(num_classes).to(device)\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"Model has {param_count:,} parameters\")\n",
    "\n",
    "            model, history, best_acc = train_model(\n",
    "                model, train_loader, val_loader, model_name, num_epochs=15\n",
    "            )\n",
    "\n",
    "            plot_training_history(history, model_name)\n",
    "\n",
    "            model.load_state_dict(torch.load(f'best_{model_name.lower()}_model.pth'))\n",
    "            final_acc = evaluate_model(model, val_loader, model_name, classes)\n",
    "\n",
    "            results[model_name] = {\n",
    "                'best_accuracy': best_acc,\n",
    "                'final_accuracy': final_acc,\n",
    "                'history': history,\n",
    "                'parameters': param_count\n",
    "            }\n",
    "\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_class': model_class.__name__,\n",
    "                'classes': classes,\n",
    "                'best_accuracy': best_acc,\n",
    "                'history': history,\n",
    "                'parameters': param_count\n",
    "            }, f'{model_name.lower()}_final.pth')\n",
    "\n",
    "            print(f\"{model_name} training completed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # After training all models, we are about to compare them\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL COMPARISON OF ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_data = []\n",
    "    for model_name, result in results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Best Accuracy (%)': f\"{result['best_accuracy']:.2f}\",\n",
    "            'Final Accuracy (%)': f\"{result['final_accuracy']:.2f}\",\n",
    "            'Parameters': f\"{result['parameters']:,}\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    if results:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        model_names = list(results.keys())\n",
    "        best_accs = [results[name]['best_accuracy'] for name in model_names]\n",
    "        final_accs = [results[name]['final_accuracy'] for name in model_names]\n",
    "\n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.bar(x - width/2, best_accs, width, label='Best Validation Accuracy', alpha=0.8)\n",
    "        plt.bar(x + width/2, final_accs, width, label='Final Validation Accuracy', alpha=0.8)\n",
    "\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Model Comparison - Air Pollution Detection')\n",
    "        plt.xticks(x, model_names)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        for i, (best, final) in enumerate(zip(best_accs, final_accs)):\n",
    "            plt.text(i - width/2, best + 0.5, f'{best:.1f}%', ha='center', va='bottom')\n",
    "            plt.text(i + width/2, final + 0.5, f'{final:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nTraining pipeline completed!\")\n",
    "    return results\n",
    "\n",
    "# Run the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
